{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCRLv2 source code: https://github.com/seanreed1111/PCRLv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 23:04:57,486 : DEBUG : __main__ : logging started\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import logging, argparse, os, pathlib\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import json, time\n",
    "import webbrowser\n",
    "import datetime\n",
    "from azure.ai.ml import MLClient\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from pathlib import Path\n",
    "from src.loggers import create_python_logger\n",
    "(Path.cwd() / 'logs').mkdir(exist_ok=True)\n",
    "logger = create_python_logger(__name__)\n",
    "logger.debug(\"logging started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "\n",
    "load_dotenv(\"~/.env\")\n",
    "\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    subscription_id=os.getenv(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.getenv(\"RESOURCE_NAME\"),\n",
    "    workspace_name=os.getenv(\"WORKSPACE_NAME\"),\n",
    "    credential=credential,\n",
    ")\n",
    "### setup\n",
    "compute = {}\n",
    "compute[\"sean-cpu-cluster-2\"] = {\"instance_type\":\"STANDARD_DS12_V2\", \"gpu\":False, \"machine_name\":\"STANDARD_DS12_V2\", \"process_count_per_instance\":1}\n",
    "compute[\"gpu-cluster-2-V100s\"] = {\"instance_type\":\"STANDARD_NC6s_v3\", \"gpu\":True, \"machine_name\":\"Telsa V100\", \"process_count_per_instance\":1}\n",
    "compute[\"gpu-cluster-2-V100s-LP\"] = {\"instance_type\":\"STANDARD_NC6s_v3\", \"gpu\":True, \"machine_name\":\"Telsa V100\", \"process_count_per_instance\":1}\n",
    "compute[\"gpu-cluster-4-V100s\"] = {\"instance_type\":\"STANDARD_NC6s_v3\", \"gpu\":True, \"machine_name\":\"Telsa V100\", \"process_count_per_instance\":1}\n",
    "compute[\"new-gpu-cluster-4-V100s-LP\"] = {\"instance_type\":\"STANDARD_NC6s_v3\", \"gpu\":True, \"machine_name\":\"Telsa V100\", \"process_count_per_instance\":1}\n",
    "compute[\"gpu-cluster-1-4xV100s-LP\"] = {\"instance_type\":\"Standard_NC24s_v3\", \"gpu\":True, \"machine_name\":\"Telsa 4xV100\", \"process_count_per_instance\":4}\n",
    "compute[\"gpu-cluster-1-4xV100s\"] = {\"instance_type\":\"Standard_NC24s_v3\", \"gpu\":True, \"machine_name\":\"Telsa 4xV100\", \"process_count_per_instance\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### INPUT ####################\n",
    "compute_target = \"gpu-cluster-2-V100s-LP\"\n",
    "##############################################################\n",
    "\n",
    "environment = \"env-PCRLv2\"\n",
    "device = 'gpu' if compute[compute_target]['gpu'] else 'cpu'\n",
    "tier = \"LowPriority\" if (\"LP\" in compute_target) else \"Dedicated\"\n",
    "instance_type = compute[compute_target][\"instance_type\"]\n",
    "process_count_per_instance = compute[compute_target][\"process_count_per_instance\"]\n",
    "### Metric Display Variables\n",
    "machine_name = compute[compute_target][\"machine_name\"]\n",
    "display_name = f\"{compute_target}\"\n",
    "\n",
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    gpu_cluster = ml_client.compute.get(compute_target)\n",
    "    print(\n",
    "        f\"You already have a cluster named {compute_target}, we'll reuse it as is.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(f\"Creating a new compute target...\")\n",
    "\n",
    "    cluster = AmlCompute(\n",
    "        name=compute_target,\n",
    "        type=\"amlcompute\",\n",
    "        size= instance_type,\n",
    "        min_instances=0,\n",
    "        max_instances=2,\n",
    "        idle_time_before_scale_down=200,\n",
    "        tier=tier, #Dedicated, LowPriority\n",
    "    )\n",
    "\n",
    "    cluster = ml_client.begin_create_or_update(cluster)\n",
    "\n",
    "print(\n",
    "    f\"AMLCompute with name {compute_target} is available\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### INPUT  ####################\n",
    "device_count = 1\n",
    "precision = 32\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "##############################################################\n",
    "## Sanity checks\n",
    "precision = 32 if not compute[compute_target]['gpu'] else precision #cpu compute must be 32\n",
    "\n",
    "\n",
    "# if cache_dataset:\n",
    "#     p0 = \"cache_dataset\"\n",
    "# elif persistent_dataset:\n",
    "#     p0 = \"persistent_dataset\"\n",
    "# else:\n",
    "#     p0 = \"\"\n",
    "# p1 = \"fastdevrun\" if fast_dev_run else \"\"\n",
    "# p2 = \"debug_loader\" if debug_get_loader else \"\"\n",
    "# p3 =\"accumulate_grad_batches\" if accumulate_grad_batches else \"\"\n",
    "# p4 = f\"overfit{overfit_batches}-batches\" if overfit_batches else \"\"\n",
    "# p5 = f\"batch-size{batch_size}-sw_batch_size{sw_batch_size}\"\n",
    "\n",
    "experiment_name = f\"{Path.cwd().stem}\"\n",
    "\n",
    "run_config = {\n",
    "    'device': device,\n",
    "    \"precision\": precision,\n",
    "    \"device_count\": device_count,\n",
    "    \"compute_target\": compute_target,\n",
    "    \"compute_target_args\": compute[compute_target],\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"environment\": environment,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"tier\": tier\n",
    "}\n",
    "p = Path(\"src/cfg/run_config.yaml\")\n",
    "p.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(p, \"w\") as f:\n",
    "    OmegaConf.save(OmegaConf.create(run_config), f=f.name)\n",
    "\n",
    "training_job = command(\n",
    "    # local path where the code is stored\n",
    "    code=\"./src\",\n",
    "    # describe the command to run the python script, with all its parameters\n",
    "    # use the syntax below to inject parameter values from code\n",
    "    command=\"\"\"python test.py \\\n",
    "        base_data_dir=${{inputs.base_data_dir}}, \\\n",
    "        base_weight_path=${{inputs.base_weight_path}}\n",
    "    \"\"\",\n",
    "    inputs={\n",
    "        \"base_data_dir\": Input(\n",
    "            type=\"uri_folder\",\n",
    "            path=\"azureml://datastores/workspaceworkingdirectory/paths/Users/sean.reed/data/base-data-dir-v1\",\n",
    "            mode=\"ro_mount\",  # use mode=\"download\" to make access faster, \"ro_mount\" if dataset is larger than VM\n",
    "        ),\n",
    "        \"base_weight_path\": Input(\n",
    "            type=\"uri_folder\",\n",
    "            path=\"azureml://datastores/workspaceworkingdirectory/paths/Users/sean.reed/PCRLv2/pretrained_weights\",\n",
    "            mode=\"ro_mount\",  # use mode=\"download\" to make access faster, \"ro_mount\" if dataset is larger than VM\n",
    "        )\n",
    "    },\n",
    "    environment=environment,\n",
    "    compute=compute_target,\n",
    "    distribution={\n",
    "        \"type\": \"PyTorch\",\n",
    "        # set process count to the number of gpus on the node\n",
    "        \"process_count_per_instance\": process_count_per_instance,\n",
    "    },\n",
    "    # set instance count to the number of nodes you want to use\n",
    "    instance_count=device_count,\n",
    "    display_name=display_name,\n",
    "    description=f\"This job is using {device_count} x  {process_count_per_instance} x{machine_name}s in environment {environment} on the {tier} tier\",\n",
    ")\n",
    "\n",
    "import webbrowser\n",
    "\n",
    "# submit the job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    training_job,\n",
    "    # Project's name\n",
    "    experiment_name=experiment_name,\n",
    ")\n",
    "\n",
    "# get a URL for the status of the job\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(returned_job.studio_url)\n",
    "# open the browser with this url\n",
    "webbrowser.open(returned_job.studio_url)\n",
    "\n",
    "# print the pipeline run id\n",
    "print(\n",
    "    f\"The pipeline details can be access programmatically using identifier: {returned_job.name}\"\n",
    ")\n",
    "# saving it for later in this notebook\n",
    "small_scale_run_id = returned_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2dd4279408478fb5fbae56646d5281dc9f0b3d3577da1b1b7537ba65e09c1ac3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
